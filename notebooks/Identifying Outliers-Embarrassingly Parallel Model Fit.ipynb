{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is XXXXXXXXXXXXXXX's notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import HiveContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be able to connect the the Hive Metastore and use the information in there about file paths and schemas from Spark. To do this, we need to create a HiveContext (Note: How this is done changes in Spark 2.0, the most recent version of Spark.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You all are only working with a single executor, so processing is going to go slowly. I've created a sample of the whole data set. I would recommend working with samples of the whole dataset in order to have work you launch complete in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hourly_pageviews_tbl = sqlContext.sql(\"SELECT * FROM u_srowen.hour_smpl_wiki_pageviews\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to transform our data into an RDD of tuples where the first value is the page name and the second values is a pd.Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_keyed_ts(row_rdd):\n",
    "    \"\"\"Transforms a dataframe into an rdd keyed by (page_name, (timestamp, count)) and\n",
    "    an associated value that is a pandas Series of counts of pageviews indexed by timestamp.\n",
    "    \"\"\"\n",
    "    select_data = split_line_data.map(lambda row: row_to_tuple(row))\n",
    "    all_as_rdd = select_data.groupByKey()\n",
    "                            .mapValues(lambda iterable: to_series(iterable.data))\n",
    "    return all_as_rdd.filter(lambda x: x[1] is not None)\n",
    "\n",
    "def row_to_tuple(row):\n",
    "    timestamp = pd.to_datetime(\"{0}-{1}-{2} {3}:00:00\".format(row.year,\n",
    "                                                              row.month,\n",
    "                                                              row.day,\n",
    "                                                              row.hour))\n",
    "    return (row.page_name, (timestamp, row.n_views))\n",
    "\n",
    "def to_series(tuples):\n",
    "    \"\"\"Transforms a list of tuples of the form (date, count) in to a pandas\n",
    "    series indexed by dt.\n",
    "    \"\"\"\n",
    "    cleaned_time_val_tuples = [tuple for tuple in tuples if not (\n",
    "        tuple[0] is pd.NaT or tuple[1] is None)]\n",
    "    if len(cleaned_time_val_tuples) > 0:\n",
    "        # change list of tuples ie [(a1, b1), (a2, b2), ...] into\n",
    "        # tuple of lists ie ([a1, a2, ...], [b1, b2, ...])\n",
    "        unzipped_cleaned_time_values = zip(*cleaned_time_val_tuples)\n",
    "        # just being explicit about what these are\n",
    "        counts = unzipped_cleaned_time_values[1]\n",
    "        timestamps = unzipped_cleaned_time_values[0]\n",
    "        # Create the series with a sorted index.\n",
    "        ret_val = pd.Series(counts, index=timestamps).sort_index()\n",
    "    else:\n",
    "        ret_val = None\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timeseries_rdd = hourly_pageviews_tbl.map(to_keyed_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that we can apply to a pandas series that will return the subset of the pandas series that satisfies Tukey's outlier criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flag_outliers(series, iqr_multiplier=1.5):\n",
    "    \"\"\"Use Tukey's boxplot criterion for outlier identification.\n",
    "    \"\"\"\n",
    "    top_quartile_cutoff = np.percentile(series.get_values(), 75)\n",
    "    bottom_quartile_cutoff = np.percentile(series.get_values(), 25)\n",
    "    # Compute interquartile range\n",
    "    iqr = top_quartile_cutoff - bottom_quartile_cutoff\n",
    "    top_outlier_cutoff = top_quartile_cutoff + iqr * iqr_multiplier\n",
    "    bottom_outlier_cutoff = bottom_quartile_cutoff - iqr * iqr_multiplier\n",
    "    return series[(series < bottom_outlier_cutoff) | (series > top_outlier_cutoff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outlier_rdd = timeseries_rdd.mapValues(flag_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outlier_rdd.take(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
