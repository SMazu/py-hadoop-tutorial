{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is XXXXXXXXXXXXXXX's notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Wiki Pageview Date\n",
    "Wikipedia makes a lot of their data publicly available, inline with their open access philosophy. It is one of my favorite example of large (note: their data is particularly large because much of it is released in formats that do not optimize for efficient representation or reducing data redundancy.), publicly available datasets, because it is an incredibly popular website that covers a ton of domains and languages. \n",
    "\n",
    "Their data is popular and people make many interesting things with it. In particular, I think [Wikitrends is a great example.](http://www.wikipediatrends.com/) Currently on their front page they are displaying an interactive plot of pageviews for presidential candidate's pages.\n",
    "\n",
    "![trending on wikitrends](img/trending-on-wikitrends.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They also provide a page-customizable view for anyone to look up view numbers for any page.\n",
    "\n",
    "![lemmy views](img/search-interface.png)\n",
    "\n",
    "This data is interesting because it tells us what people are looking up on wikipedia. From this we can infer that their mind is on the topic. Talk about a look into cultural Zeitgeist!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use pageveiw data where the wikimedia team has already removed as many identifiable bot requests as possible. Documentation on this data is avaiable here [available here.](https://dumps.wikimedia.org/other/pagecounts-raw/) The data is made available in hourly files and the urls are first split by year, then by month.\n",
    "\n",
    "### dumps.wikimedia.org/other/pageviews/\n",
    "![screen shot of file index1](img/pgvw1.png)\n",
    "\n",
    "### dumps.wikimedia.org/other/pageviews/2016/\n",
    "![screen shot of file index1](img/pgvw2.png)\n",
    "\n",
    "### dumps.wikimedia.org/other/pageviews/2016/2016-01/\n",
    "![screen shot of file index1](img/pgvw3.png)\n",
    "\n",
    "[Files reside here](https://dumps.wikimedia.org/other/pageviews/)\n",
    "\n",
    "[Docs on pageview stats](https://en.wikipedia.org/wiki/Wikipedia:Pageview_statistics)\n",
    "\n",
    "[Other, related data](https://dumps.wikimedia.org/other/analytics/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our strategy for moving this data into HDFS is to download it locally and the then use the hdfs client 'put' command to move the data from local to HDFS. To download the data we will:\n",
    "\n",
    "* Send and HTTP request to get a page listing all files in a certain month.\n",
    "* Parse the returned HTML to retrieve all file names.\n",
    "* Filter file names by day so we dont spend our entire lives downloading data (ie. only download a days worth of data at a time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will parse the html with Beautiful soup, NOT A REGEX:\n",
    "\n",
    "![screen shot of the stack overflow QA with the amazing characters](img/stackoverflow.png)\n",
    "\n",
    "We will:\n",
    "* retreive the index page for the hosted files as text. These will look like:\n",
    "\n",
    "\n",
    "    <li>\n",
    "        <a href='pageviews20160101-010000.gz'>\n",
    "        ...etc...\n",
    "    </li>\n",
    "\n",
    "* iterate through all links (ie tag 'a') and grab the associated urls (labeled 'href')\n",
    "* yield the fully qualified url back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pageviews(year, month, day):\n",
    "    pageviews_url = 'https://dumps.wikimedia.org/other/pageviews/{0}/{0}-{1}/'.format(year, month)\n",
    "    soup = BeautifulSoup(requests.get(pageviews_url).text)\n",
    "    for a in soup.find_all('a'):\n",
    "        if 'pageviews-{0}{1}{2}'.format(year, month, day) in a['href']:\n",
    "            yield pageviews_url + a['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the fully qualified url we will use the reuqests package to retreive the contents of the file hosted there. Pretty straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_file(url):\n",
    "    req = requests.get(url, stream=True)\n",
    "    local_filename = url.split(\"/\")[-1]\n",
    "    with open('data/' + local_filename, 'wb') as f:\n",
    "        for chunk in req.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "                f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all hourly files matching the pattern we expect for January 1st 2016, download it and write the first one to local disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_pageviews = [n for n in get_pageviews('2016', '01', '01')]\n",
    "write_file(all_pageviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving data to HDFS\n",
    "\n",
    "HDFS provides a CLI for common operations. We would like to move our data withut leaving python. We have two options to do this:\n",
    "\n",
    "* Use the subprocess package to shellout commands to the HDFS CLI, and run from a machine where that CLI is installed.\n",
    "* Use Ibis' HDFS client (a wrapper around WebHDFS) to move the data.\n",
    "\n",
    "Using subprocess always feels like a hack, so let's go with the more straightforward HDFS connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# System independent way to join paths\n",
    "local_data_path = os.path.join(os.getcwd(), \"pageviews-gz\")\n",
    "\n",
    "def mv_files(filename, hdfs_dir, hdfs_conn):\n",
    "    dir_name = hdfs_dir + filename[:-3]\n",
    "    hdfs_conn.mkdir(dir_name)\n",
    "    filepathtarget = '/'.join([dir_name, filename])\n",
    "    hdfs_conn.put(filepathtarget, os.path.join(local_data_path, filename))\n",
    "    return dir_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data resides in HDFS, we would like to transform it so that we have fast access for analysis. The current format has a few problems:\n",
    "\n",
    "* Information contained in file name.\n",
    "* Many GZipped files.\n",
    "* The underlying files are text.\n",
    "* The text files contain space delimited data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, especially with analytics, we want:\n",
    "\n",
    "* Scans/aggregations over columns.\n",
    "* Good, splittable compression.\n",
    "* A binary format, performant encodings.\n",
    "* Designed with compelx nested data sructures in mind.\n",
    "\n",
    "My default recommendation is Praquet. It is a binary file format designed for Hadoop and fast column oriented aggregations. We can use it in conjunction with a splittable compresseion codec like LZO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this transfromation with either Ibis (Impala) or (Py)Spark. Parquet files generated by spark are not compatible with Impala, so, for the sake of our sanity we will use Ibis to do this transformation. The steps will be:\n",
    "\n",
    "* Read in files as a temporary table in Ibis.\n",
    "* Extract time information from file path and add to table.\n",
    "* Insert data into permanent table in Impala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data from an HDFS directory\n",
    "\n",
    "We can define the schema of the files we want to read in and create a temporary table for Ibis to read from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_schema = ibis.schema([('project_name', 'string'),\n",
    "                           ('page_name', 'string'),\n",
    "                           ('n_views', 'int64'),\n",
    "                           ('n_bytes', 'int64')])\n",
    "\n",
    "\n",
    "tmp_table = ibis_conn.delimited_file(hdfs_dir=data_dir,\n",
    "                                     schema=file_schema,\n",
    "                                     delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create New Columns\n",
    "\n",
    "We can create new named columns using the 'mutate' method. Here, year, month, day, and hour are string values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_datetime(filename):\n",
    "    _, date_str, time_str = filename.split(\"-\")\n",
    "    year = date_str[:4]\n",
    "    month = date_str[4:6]\n",
    "    day = date_str[-2:]\n",
    "    hour = time_str[:2]\n",
    "    return year, month, day, hour\n",
    "\n",
    "year, month, day, hour = extract_datetime(data_dir.split(\"/\")[-1])\n",
    "\n",
    "# create a column for year, month, day and hour.\n",
    "tmp_w_time = tmp_table.mutate(year=year,\n",
    "                              month=month,\n",
    "                              day=day,\n",
    "                              hour=hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Metastore\n",
    "Ibis allows us to intergoate the hive metastore. We can determine if databases or tables exists by using functions defined directly on the ibis_connection.\n",
    "\n",
    "It is useful for us to determine if a database exists and then create it if it does not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not ibis_conn.exists_database(db_name):\n",
    "    ibis_conn.create_database(db_name)\n",
    "\n",
    "working_db = ibis_conn.database(db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Data\n",
    "We can then create a table from an ibis expression or insert more data into a table with the same schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'wiki_pageviews' in working_db.tables:\n",
    "    ibis_conn.insert('wiki_pageviews', tmp_w_time, database=db_name)\n",
    "else:\n",
    "    ibis_conn.create_table('wiki_pageviews', obj=tmp_w_time,\n",
    "                           database=db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, wrap this all up in a function so we can use it in a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gz_2_data_insert(data_dir, ibis_conn, db_name):\n",
    "    tmp_table = ibis_conn.delimited_file(hdfs_dir=data_dir,\n",
    "                                  schema=file_schema,\n",
    "                                  delimiter=' ')\n",
    "    year, month, day, hour = extract_datetime(data_dir.split(\"/\")[-1])\n",
    "    # create a column named time\n",
    "    tmp_w_time = tmp_table.mutate(year=year, month=month, day=day, hour=hour)\n",
    "\n",
    "    working_db = safe_get_db(ibis_conn, db_name)\n",
    "    if 'wiki_pageviews' in working_db.tables:\n",
    "        ibis_conn.insert('wiki_pageviews', tmp_w_time, database=db_name)\n",
    "    else:\n",
    "        ibis_conn.create_table('wiki_pageviews', obj=tmp_w_time,\n",
    "                               database=db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "local_files = os.listdir(local_data_path)\n",
    "# There is an impala deamon on eachmachine. You likely want change\n",
    "# the host name to match the machine you are working on.\n",
    "hdfs_conn = ibis.hdfs_connect(host='cdh1.c.guerilla-python.internal')\n",
    "db_name = 'u_srowen.wikipageviews'\n",
    "hdfs_dir = '/user/srowen'\n",
    "\n",
    "hdfs_gz_dirs = [mv_files(filename, hdfs_dir, hdfs_conn) for filename in local_files]\n",
    "[gz_2_data_insert(data_dir, ibis_conn, db_name) for data_dir in hdfs_gz_dirs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
